import streamlit as st
import tempfile
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Session state initialization
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None

# Functions
@st.cache_resource
def load_embeddings():
    """Load Vietnamese embedding model"""
    return HuggingFaceEmbeddings(
        model_name="bkai-foundation-models/vietnamese-bi-encoder"
    )

def get_llm():
    """
    Get Groq LLM instance
    API key from environment variable or Streamlit secrets
    """
    api_key = os.getenv("GROQ_API_KEY") or st.secrets.get("GROQ_API_KEY")
    
    if not api_key:
        st.error("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh Groq API key!")
        st.info("""
        **C√°ch l·∫•y API key mi·ªÖn ph√≠:**
        1. Truy c·∫≠p: https://console.groq.com
        2. ƒêƒÉng k√Ω t√†i kho·∫£n (mi·ªÖn ph√≠)
        3. T·∫°o API key
        4. Th√™m v√†o file `.env`: `GROQ_API_KEY=gsk_xxx`
        
        **Ho·∫∑c trong Streamlit Cloud:**
        Settings ‚Üí Secrets ‚Üí Th√™m `GROQ_API_KEY = "gsk_xxx"`
        """)
        st.stop()
    
    return ChatGroq(
        model="llama-3.1-70b-versatile",  # Model 70B, m·∫°nh h∆°n Vicuna-7B nhi·ªÅu
        temperature=0,
        api_key=api_key,
        max_tokens=512
    )

def process_pdf(uploaded_file):
    """Process uploaded PDF and create RAG chain"""
    
    # Save uploaded file to temp location
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    try:
        # Load PDF
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        
        # Split documents semantically
        semantic_splitter = SemanticChunker(
            embeddings=st.session_state.embeddings,
            buffer_size=1,
            breakpoint_threshold_type="percentile", 
            breakpoint_threshold_amount=95,
            min_chunk_size=500,
            add_start_index=True
        )
        
        docs = semantic_splitter.split_documents(documents)
        
        # Create vector database
        vector_db = Chroma.from_documents(
            documents=docs, 
            embedding=st.session_state.embeddings
        )
        retriever = vector_db.as_retriever()
        
        # Get prompt template
        prompt = hub.pull("rlm/rag-prompt")
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        # Create RAG chain with Groq
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt 
            | get_llm()  # ‚Üê D√πng Groq API thay v√¨ local model
            | StrOutputParser()
        )
        
        return rag_chain, len(docs)
    
    finally:
        # Cleanup temp file
        if os.path.exists(tmp_file_path):
            os.unlink(tmp_file_path)

# UI
def main():
    st.set_page_config(
        page_title="PDF RAG Assistant", 
        page_icon="üìö",
        layout="wide"
    )
    
    st.title("üìö PDF RAG Assistant")

    st.markdown("""
    **·ª®ng d·ª•ng AI gi√∫p b·∫°n h·ªèi ƒë√°p tr·ª±c ti·∫øp v·ªõi n·ªôi dung t√†i li·ªáu PDF b·∫±ng ti·∫øng Vi·ªát**

    **C√°ch s·ª≠ d·ª•ng ƒë∆°n gi·∫£n:**
    1. **Upload PDF** ‚Üí Ch·ªçn file PDF t·ª´ m√°y t√≠nh v√† nh·∫•n "X·ª≠ l√Ω PDF"  
    2. **ƒê·∫∑t c√¢u h·ªèi** ‚Üí Nh·∫≠p c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu v√† nh·∫≠n c√¢u tr·∫£ l·ªùi ngay l·∫≠p t·ª©c

    ---
    """)

    # Load embeddings
    if not st.session_state.models_loaded:
        with st.spinner("üîÑ ƒêang t·∫£i embedding model..."):
            st.session_state.embeddings = load_embeddings()
            st.session_state.models_loaded = True
        st.success("‚úÖ Embedding model ƒë√£ s·∫µn s√†ng!")

    # Upload PDF
    uploaded_file = st.file_uploader("üìÑ Upload file PDF", type="pdf")
    
    if uploaded_file and st.button("üîÑ X·ª≠ l√Ω PDF", type="primary"):
        with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω PDF..."):
            try:
                st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)
                st.success(f"‚úÖ Ho√†n th√†nh! ƒê√£ chia th√†nh {num_chunks} chunks")
            except Exception as e:
                st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω PDF: {str(e)}")

    # Q&A
    st.markdown("---")
    
    if st.session_state.rag_chain:
        question = st.text_input(
            "üí¨ ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu:", 
            placeholder="VD: T√†i li·ªáu n√†y n√≥i v·ªÅ g√¨?"
        )
        
        if question:
            with st.spinner("ü§î ƒêang suy nghƒ©..."):
                try:
                    output = st.session_state.rag_chain.invoke(question)
                    
                    # Extract answer
                    if 'Answer:' in output:
                        answer = output.split('Answer:')[1].strip()
                    else:
                        answer = output.strip()
                    
                    st.markdown("### üí° Tr·∫£ l·ªùi:")
                    st.write(answer)
                    
                except Exception as e:
                    st.error(f"‚ùå L·ªói khi tr·∫£ l·ªùi: {str(e)}")
    else:
        st.info("‚ÑπÔ∏è Vui l√≤ng upload v√† x·ª≠ l√Ω PDF tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi")

if __name__ == "__main__":
    main()